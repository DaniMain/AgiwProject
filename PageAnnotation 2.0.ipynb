{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SCUSATE PER I NOMI UN PO' A CAZZO DI CANE MA HO FATTO PER SBRIGARMI\n",
    "\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import lxml.html\n",
    "from lxml import etree\n",
    "import xml.etree.ElementTree as ET\n",
    "import string\n",
    "import csv\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def AbsolutePaths(tags):\n",
    "    tmp=''\n",
    "    for tag in tags:\n",
    "        tmp+=tag.tag+'/'\n",
    "    return str(tmp)    \n",
    "\n",
    "def xpath_generator(C):\n",
    "  X = []\n",
    "  for link in C:\n",
    "    #ritorna la get\n",
    "    response = requests.get(link)\n",
    "    #ritorna l'elemento html della pagina\n",
    "    tree = lxml.html.fromstring(response.text)\n",
    "    #ritorna la lista dei nodi foglia\n",
    "    childs = tree.xpath('//*[not(child::*)]')\n",
    "    #funzione che calcola il path assoluto radice-foglia come stringa\n",
    "    #calcolo della lista che contiene gli xpath assoluti alle foglie della pagina \n",
    "    for child in childs:\n",
    "      #ciascun elemento in X è una quadrupla (implementata come una lista di 4 elementi) formata da:\n",
    "      #url della pagina, xpath di ciascun nodo della pagina, contenuto del nodo, peso del nodo\n",
    "      xpath_element = []\n",
    "      tags = child.xpath('ancestor::node()')\n",
    "      path = AbsolutePaths(tags)\n",
    "      path += str(child.tag)\n",
    "      contenuto = child.text_content()\n",
    "      xpath_element.append(link)\n",
    "      xpath_element.append(path)\n",
    "      xpath_element.append(contenuto)\n",
    "      xpath_element.append(0)\n",
    "      X.append(xpath_element)\n",
    "  return X\n",
    "\n",
    "#Ricerca la frequenza e la frequenza del contenuto di un certo xpath\n",
    "def find_f(url, xpath, contenuto, lista):\n",
    "  frequenza = 1\n",
    "  f_contenuto = 1\n",
    "  #lista dei url già visti nella ricerca\n",
    "  url_visti = []\n",
    "  url_visti.append(url)\n",
    "  for new_path in lista:\n",
    "    if new_path[0] not in url_visti:\n",
    "      if xpath == new_path[1]:\n",
    "        url_visti.append(new_path[0])\n",
    "        frequenza += 1\n",
    "        if contenuto == new_path[2]:\n",
    "          f_contenuto += 1  \n",
    "  return frequenza, f_contenuto\n",
    "\n",
    "#lista = [(url,xpath,contenuto,peso), (....), (....), .... ]\n",
    "def calcolo_peso_xpath(lista, num_pagine):\n",
    "  for xpath in lista:\n",
    "    #prendo xpath\n",
    "    x = xpath[1]\n",
    "    url = xpath[0]\n",
    "    contenuto = xpath[2]\n",
    "    #calcolo la frequenza e la frequenza del contenuto\n",
    "    frequenza, f_contenuto = find_f(url, x, contenuto, lista)\n",
    "    #calcolo della informatività\n",
    "    info = 1 - (f_contenuto/num_pagine)\n",
    "    #calcolo del peso del xpath\n",
    "    peso = frequenza * info\n",
    "    #inserisco il peso nella lista\n",
    "    xpath[3] = peso\n",
    "    \n",
    "def somma_pesi_per_pagina(X):\n",
    "  url_pesi = {}\n",
    "  for e in X:\n",
    "    k = e[0]\n",
    "    if k not in url_pesi.keys():\n",
    "      url_pesi[k] = e[3]\n",
    "    else:\n",
    "      url_pesi[k] += e[3]\n",
    "  return d\n",
    "\n",
    "#Trova la pagina (ritornando il suo url) con la somma dei pesi maggiore\n",
    "def find_p(X, M):\n",
    "  calcolo_peso_xpath(X, M)\n",
    "  pagine_con_pesi = somma_pesi_per_pagina(X)\n",
    "  peso_max = 0\n",
    "  pagina_max = ''\n",
    "  for k in pagine_con_pesi.keys():\n",
    "    peso_pagina = pagine_con_pesi[k]\n",
    "    if peso_pagina > peso_max:\n",
    "      pagina_max = k\n",
    "      peso_max = peso_pagina\n",
    "  return pagina_max\n",
    "\n",
    "#Crea un dizionario con chiave xpath e valore il contenuto\n",
    "def build_dict(X,p):\n",
    "  d = {}\n",
    "  for e in X:\n",
    "    if e[0] == p:\n",
    "      if e[1] not in d:\n",
    "        xpath = e[1]\n",
    "        content = e[2]\n",
    "        d[xpath] = content\n",
    "  return d\n",
    "\n",
    "#Rimuove la pagina da X e gli Xpath associati ad essa\n",
    "def remove_p(X, p):\n",
    "  X1 = []\n",
    "  for e in X:\n",
    "    if e[0] != p:\n",
    "      X1.append(e)\n",
    "  #elimino gli xpath contenuto in p nelle altre pagine\n",
    "  X2 = []\n",
    "  dict_xpath_content = build_dict(X,p)\n",
    "  for e in X1:\n",
    "    if e[1] not in dict_xpath_content:\n",
    "      X2.append(e)\n",
    "    elif e[2] != dict_xpath_content[e[1]]:\n",
    "        X2.append(e)  \n",
    "  return X2\n",
    "\n",
    "#Main\n",
    "def page_annotation(C, K):\n",
    "  X = xpath_generator(C)\n",
    "  S = []\n",
    "  while X!=[] and len(S)<=K:\n",
    "    p = find_p(X, len(C))\n",
    "    S.append(p)\n",
    "    X = remove_p(X, p)\n",
    "  return S"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
